async function processAudioWithAI(audioFile, planType, fingerprint, env, request) {
  try {
    // Read file bytes
    const audioBuffer = await audioFile.arrayBuffer();

    // --- Transcribe with Cloudflare Workers AI Whisper ---
    const whisper = await transcribeWithWorkersAI(audioBuffer, env);

    // Normalize transcription shape
    const transcription = {
      text: whisper.text || whisper.transcript || '',
      language: (whisper.language || whisper.detected_language || 'auto'),
      segments: Array.isArray(whisper.segments) ? whisper.segments.map(s => ({
        start: Number(s.start ?? s.start_time ?? 0),
        end: Number(s.end ?? s.end_time ?? (s.start ?? 0) + 1),
        text: String(s.text ?? ''),
        confidence: Number(s.confidence ?? 0.9)
      })) : [{ start: 0, end: Math.min(60, Math.ceil(audioBuffer.byteLength / (44100*2))), text: String(whisper.text || ''), confidence: 0.9 }]
    };

    const detectedLanguages = extractLanguagesFromTranscription(transcription.text);
    const normalizedLanguages = normalizeLangs([transcription.language, ...detectedLanguages]);

    // Profanity detection over segments
    const profanityResults = await findProfanityTimestamps(transcription, normalizedLanguages, env);

    // Produce preview/full audio objects in R2 and signed links
    const audioOutputs = await generateAudioOutputs(
      audioBuffer,
      profanityResults,
      planType,
      getPreviewDuration(planType),
      fingerprint,
      env,
      audioFile.type,
      audioFile.name,
      request
    );

    return {
      success: true,
      previewUrl: audioOutputs.previewUrl,
      fullAudioUrl: audioOutputs.fullAudioUrl,
      languages: normalizedLanguages,
      profanityFound: profanityResults.timestamps?.length || 0,
      transcription: planType !== 'free' ? transcription : null,
      quality: getQualityForPlan(planType),
      watermarkId: audioOutputs.watermarkId
    };
  } catch (error) {
    console.error('AI processing error:', error);
    return { success: false, error: 'AI processing failed', details: error.message };
  }
}

async function transcribeWithWorkersAI(audioBuffer, env) {
  // Workers AI binding must exist in wrangler.toml as: [ai] binding = "AI"
  if (!env.AI || !env.AI.run) {
    throw new Error('Workers AI binding not configured (missing [ai] binding = "AI")');
  }

  // Model name: use Cloudflare Whisper; fall back aliases for compatibility
  const models = [
    '@cf/openai/whisper',          // primary
    '@cf/openai/whisper-1',        // alias some accounts expose
    '@cf/openai/whisper-large-v3'  // legacy alias
  ];

  // Convert to ArrayBuffer -> Uint8Array for model input
  const bytes = new Uint8Array(audioBuffer);

  let lastErr;
  for (const model of models) {
    try {
      const result = await env.AI.run(model, {
        // Most Workers AI audio models accept raw PCM/encoded bytes as `audio` or `audio_file`.
        // Provide both keys for broader compatibility.
        audio: [...bytes],
        audio_file: [...bytes],
        // Helpful flags; models ignore unknown keys safely.
        detect_language: true,
        temperature: 0
      });

      // Expected shapes: { text, language, segments } or nested under result
      if (!result) throw new Error('Empty response from Workers AI');
      const r = (result.result || result);
      if (typeof r.text === 'string' || Array.isArray(r.segments)) return r;

      // Some deployments return { output: { text, segments } }
      if (r.output && (typeof r.output.text === 'string' || Array.isArray(r.output.segments))) return r.output;

      // Fallback: stringify and attempt to parse text
      const asText = typeof r === 'string' ? r : (typeof r.text === 'string' ? r.text : '');
      return { text: asText };
    } catch (e) {
      lastErr = e;
      console.warn('Whisper model failed', model, e?.message || e);
      continue;
    }
  }
  throw lastErr || new Error('All Whisper model attempts failed');
}
